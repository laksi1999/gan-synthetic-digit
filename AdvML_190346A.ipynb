{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYBQ0Kr1yUFL",
        "outputId": "1804b91f-7477-4ed5-92a1-7aa55960d41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/AdvML/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRcssqlG9d-b",
        "outputId": "fe852efb-f727-45d6-c033-84dd71a3dee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/AdvML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "-pbMWzwzywDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "metadata": {
        "id": "1SkJcaAh0xT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='.', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "a1-NosgOzXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def de_norm(image):\n",
        "  return image * 0.5 + 0.5"
      ],
      "metadata": {
        "id": "c44VrgTWHqjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmjpduO-1JhJ",
        "outputId": "e2e4afc2-0972-425c-852e-cf9c81de180b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wh = 28 * 28"
      ],
      "metadata": {
        "id": "MstowoqV1RH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(wh, 128),\n",
        "    nn.LeakyReLU(0.01),\n",
        "    nn.Linear(128, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "D.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDkCR-jM0_Dl",
        "outputId": "f5f8cd77-d256-47ba-e741-3ad30878cf00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.01)\n",
              "  (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (3): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)"
      ],
      "metadata": {
        "id": "SVHvPZPD3otX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 100"
      ],
      "metadata": {
        "id": "hM5dV3Bo4Gm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nn.Sequential(\n",
        "    nn.Linear(z_dim, 128),\n",
        "    nn.LeakyReLU(0.01),\n",
        "    nn.Linear(128, wh),\n",
        "    nn.Tanh()\n",
        ")\n",
        "\n",
        "G.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaKF6Dlc32fn",
        "outputId": "238a7316-2731-4ea6-ffbb-7e0dbeb60d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=100, out_features=128, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.01)\n",
              "  (2): Linear(in_features=128, out_features=784, bias=True)\n",
              "  (3): Tanh()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
      ],
      "metadata": {
        "id": "2ZCRrNvR8RUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discriminator(images):\n",
        "  images = images.to(device)\n",
        "  y = torch.ones(batch_size, 1).to(device)\n",
        "  y_hat = D(images)\n",
        "  d_real_loss = d_criterion(y_hat, y)\n",
        "  real_score = y_hat\n",
        "\n",
        "  z = torch.randn(batch_size, z_dim).to(device)\n",
        "  fake_images = G(z)\n",
        "  fake_images = fake_images.to(device)\n",
        "  y = torch.zeros(batch_size, 1).to(device)\n",
        "  y_hat = D(fake_images)\n",
        "  d_fake_loss = d_criterion(y_hat, y)\n",
        "  fake_score = y_hat\n",
        "\n",
        "  d_loss = d_real_loss + d_fake_loss\n",
        "  d_optimizer.zero_grad()\n",
        "  g_optimizer.zero_grad()\n",
        "  d_loss.backward()\n",
        "  d_optimizer.step()\n",
        "\n",
        "  return d_loss.item(), real_score, fake_score"
      ],
      "metadata": {
        "id": "71fCGaeG8cvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator():\n",
        "  z = torch.randn(batch_size, z_dim).to(device)\n",
        "  fake_images = G(z)\n",
        "  fake_images = fake_images.to(device)\n",
        "  y = torch.ones(batch_size, 1).to(device)\n",
        "  y_hat = D(fake_images)\n",
        "  g_loss = d_criterion(y_hat, y)\n",
        "\n",
        "  d_optimizer.zero_grad()\n",
        "  g_optimizer.zero_grad()\n",
        "  g_loss.backward()\n",
        "  g_optimizer.step()\n",
        "\n",
        "  return g_loss.item()"
      ],
      "metadata": {
        "id": "ngz8sSqB9TY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 300"
      ],
      "metadata": {
        "id": "d3YkuVDPHRNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i, (images, _) in enumerate(train_loader):\n",
        "    images = images.reshape(batch_size, -1)\n",
        "\n",
        "    d_loss, real_score, fake_score = train_discriminator(images)\n",
        "    g_loss = train_generator()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    print(f'epoch [{epoch + 1}/{num_epochs}], step [{i + 1}/{len(train_loader)}], d_loss: {d_loss:.2f}, g_loss: {g_loss:.2f}, real_score: {real_score.mean().item():.2f}, fake_score: {fake_score.mean().item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92sNYZ7pGEOk",
        "outputId": "43771524-bf45-4c01-accd-a84e5c5b4d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/300], step [600/600], d_loss: 1.05, g_loss: 0.81, real_score: 0.72, fake_score: 0.51\n",
            "epoch [2/300], step [600/600], d_loss: 0.63, g_loss: 1.52, real_score: 0.75, fake_score: 0.28\n",
            "epoch [3/300], step [600/600], d_loss: 1.38, g_loss: 0.80, real_score: 0.57, fake_score: 0.54\n",
            "epoch [4/300], step [600/600], d_loss: 1.00, g_loss: 1.01, real_score: 0.62, fake_score: 0.39\n",
            "epoch [5/300], step [600/600], d_loss: 1.05, g_loss: 1.01, real_score: 0.62, fake_score: 0.41\n",
            "epoch [6/300], step [600/600], d_loss: 1.35, g_loss: 0.73, real_score: 0.54, fake_score: 0.50\n",
            "epoch [7/300], step [600/600], d_loss: 1.11, g_loss: 0.96, real_score: 0.63, fake_score: 0.43\n",
            "epoch [8/300], step [600/600], d_loss: 1.37, g_loss: 0.83, real_score: 0.55, fake_score: 0.50\n",
            "epoch [9/300], step [600/600], d_loss: 0.70, g_loss: 1.38, real_score: 0.71, fake_score: 0.27\n",
            "epoch [10/300], step [600/600], d_loss: 0.88, g_loss: 1.19, real_score: 0.68, fake_score: 0.36\n",
            "epoch [11/300], step [600/600], d_loss: 1.38, g_loss: 0.82, real_score: 0.49, fake_score: 0.45\n",
            "epoch [12/300], step [600/600], d_loss: 1.29, g_loss: 0.81, real_score: 0.55, fake_score: 0.47\n",
            "epoch [13/300], step [600/600], d_loss: 1.18, g_loss: 0.98, real_score: 0.58, fake_score: 0.43\n",
            "epoch [14/300], step [600/600], d_loss: 1.21, g_loss: 1.07, real_score: 0.59, fake_score: 0.43\n",
            "epoch [15/300], step [600/600], d_loss: 1.09, g_loss: 1.21, real_score: 0.62, fake_score: 0.37\n",
            "epoch [16/300], step [600/600], d_loss: 1.24, g_loss: 1.08, real_score: 0.58, fake_score: 0.43\n",
            "epoch [17/300], step [600/600], d_loss: 1.82, g_loss: 0.71, real_score: 0.44, fake_score: 0.55\n",
            "epoch [18/300], step [600/600], d_loss: 1.13, g_loss: 1.10, real_score: 0.63, fake_score: 0.42\n",
            "epoch [19/300], step [600/600], d_loss: 0.46, g_loss: 2.31, real_score: 0.82, fake_score: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = nn.Sequential(\n",
        "    nn.Conv2d(1, 36, kernel_size=5, stride=1, padding=2),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(36, 32, kernel_size=5, stride=1, padding=2),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 7 * 7, 10)\n",
        ")\n",
        "\n",
        "C.to(device)"
      ],
      "metadata": {
        "id": "rMuGdsAVLL04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_criterion = nn.CrossEntropyLoss()\n",
        "c_optimizer = torch.optim.Adam(C.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "dsnue72kLe4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "Dck3I_FZLxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    y_hat = C(images)\n",
        "    c_loss = c_criterion(y_hat, labels)\n",
        "\n",
        "    c_optimizer.zero_grad()\n",
        "    c_loss.backward()\n",
        "    c_optimizer.step()\n",
        "\n",
        "  y_pred = torch.max(y_hat, 1)[1].data.squeeze()\n",
        "  accuracy = (labels == y_pred).sum().item() / float(labels.size(0))\n",
        "\n",
        "  print(f'epoch [{epoch + 1}/{num_epochs}], step [{i + 1}/{len(train_loader)}], c_loss: {c_loss.item():.2f}, accuracy_score: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "xsxVY-5NLsw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('D.pkl', 'wb') as f:\n",
        "  pickle.dump(D, f)\n",
        "\n",
        "with open('G.pkl', 'wb') as f:\n",
        "  pickle.dump(G, f)\n",
        "\n",
        "with open('C.pkl', 'wb') as f:\n",
        "  pickle.dump(C, f)"
      ],
      "metadata": {
        "id": "SzDUaltOM9rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "bs = 100\n",
        "\n",
        "z = torch.randn(bs, z_dim).to(device)\n",
        "fake_images = G(z).reshape(bs, 1, 28, 28)\n",
        "\n",
        "for i, (fake_image, zi) in enumerate(zip(fake_images, z)):\n",
        "  f_name = str(i + 1).zfill(3)\n",
        "\n",
        "  save_image(de_norm(fake_image), os.path.join('Fake_Digits', f'{f_name}.png'))\n",
        "\n",
        "  with open(os.path.join('Fake_Digits', f'{f_name}.txt'), 'w') as f:\n",
        "    f.write(str(zi.tolist()))"
      ],
      "metadata": {
        "id": "3UCRfD9WNWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "S1 = datasets.ImageFolder(root='Fake_Digits', transform=transform)\n",
        "\n",
        "for i in range(len(S1)):\n",
        "  img_path, label = S1.imgs[i]\n",
        "  folder_name = os.path.basename(os.path.dirname(img_path))\n",
        "  S1.samples[i] = (img_path, int(folder_name))\n",
        "\n",
        "s1_loader = DataLoader(dataset=S1, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "BD_0pOd-SW4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dl in [test_loader, s1_loader]:\n",
        "  with torch.no_grad():\n",
        "    n_samples, n_correct = 0, 0\n",
        "\n",
        "    for images, labels in dl:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      y_hat = C(images)\n",
        "      y_pred = torch.max(y_hat, 1)[1].data.squeeze()\n",
        "\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (labels == y_pred).sum().item()\n",
        "\n",
        "      break\n",
        "\n",
        "    accuracy = n_correct / n_samples\n",
        "    print(accuracy)"
      ],
      "metadata": {
        "id": "SGmN1kveRmhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ebe112-be28-4400-a0ea-d0c35d9fdf64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NnOVC3C9G5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}